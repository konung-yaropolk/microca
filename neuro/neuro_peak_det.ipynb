{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a49ac22",
   "metadata": {},
   "source": [
    "# Peaks detection and features measurment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import numpy as np\n",
    "import numpy.polynomial.polynomial as poly\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy import stats\n",
    "from scipy import ndimage\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16e68eba",
   "metadata": {},
   "source": [
    "## Preprocessing & parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65d06f4c",
   "metadata": {},
   "source": [
    "#### Custom func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_prof_to_arr(input_df):\n",
    "    \"\"\" Normilize raw Ca profiles in 0-1 range and convert data frame to numpy array\n",
    "    Return:\n",
    "        - np array (dims - [component, raw_prof, df_prof, norm_prof])\n",
    "        - dict ({component:raw_prof})\n",
    "\n",
    "    \"\"\"\n",
    "    output_arr = []\n",
    "    output_dict = {}\n",
    "    comp_list = np.array(input_df.comp.unique())\n",
    "    for comp in comp_list:\n",
    "        comp_raw_prof = np.array(input_df['profile_raw'][input_df['comp'] == comp])\n",
    "        comp_df_prof = np.array(input_df['profile_ddf'][input_df['comp'] == comp])\n",
    "        comp_norm_prof = preprocessing.minmax_scale(comp_raw_prof)\n",
    "        output_arr.append([comp_raw_prof, comp_df_prof, comp_norm_prof])\n",
    "        output_dict.update({comp:comp_raw_prof})\n",
    "    return np.asarray(output_arr), output_dict\n",
    "\n",
    "def ctrl_plot(input_df):\n",
    "    # raw profiles view\n",
    "    fig=px.line(input_df,\n",
    "                y='profile_ddf',\n",
    "                x='time',\n",
    "                color='comp',\n",
    "                animation_frame=\"comp\",\n",
    "                title='Individual components ΔF/F profiles')\n",
    "    fig.update(layout_yaxis_range = [min(input_df.profile_ddf), max(input_df.profile_ddf)])\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def prof_peaks_plot(input_profile, peaks_i, peaks_prop, x_time=None, add_profile=None):\n",
    "    \"\"\" SciPy find_peaks results for individual profile plotting,\n",
    "    for 0-1 range scaling profile only\n",
    "\n",
    "    \"\"\"\n",
    "    # features calc\n",
    "    if x_time is None:\n",
    "        x_time = np.linspace(0, input_profile.shape[0], input_profile.shape[0])\n",
    "    prom = peaks_prop['prominences']\n",
    "    prom_h = input_profile[peaks_i] - prom\n",
    "    width_w = np.sort(np.asarray([*peaks_prop['left_bases'], *peaks_prop['right_bases']], dtype=int))\n",
    "    width_pairs =  np.asarray(list(zip(peaks_prop['left_bases'], peaks_prop['right_bases'])), dtype=int)\n",
    "    fwhm_y = peaks_prop['width_heights']\n",
    "    fwhm_l = x_time[np.asarray(peaks_prop['left_ips'], dtype=int)]\n",
    "    fwhm_r = x_time[np.asarray(peaks_prop['right_ips'], dtype=int)]\n",
    "\n",
    "    # plotting\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(x_time, input_profile)\n",
    "    plt.plot(x_time[peaks_i], input_profile[peaks_i], 'x', color='red')\n",
    "    plt.plot(x_time[width_w], input_profile[width_w], '.', color='red')\n",
    "    plt.vlines(x=x_time[peaks_i], ymin=prom_h, ymax=input_profile[peaks_i], color='red')\n",
    "    plt.hlines(y=fwhm_y, xmin=fwhm_l, xmax=fwhm_r, color='red')\n",
    "    plt.hlines(y=.5, xmin=0, xmax=np.max(x_time), linestyles='--', color='k')\n",
    "    plt.fill_between(x=x_time,\n",
    "                     y1=input_profile,\n",
    "                     y2=.5,\n",
    "                     color='y',\n",
    "                     alpha=.2,\n",
    "                     where=input_profile>=.5)\n",
    "    for peak_width in width_pairs:\n",
    "        plt.fill_between(x= x_time[peak_width[0]:peak_width[1]], \n",
    "                         y1= input_profile[peak_width[0]:peak_width[1]], \n",
    "                         color= \"red\",\n",
    "                         alpha= 0.2)\n",
    "    if add_profile is not None:\n",
    "        plt.plot(x_time, add_profile, color='g', alpha=.25)\n",
    "    # plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def cascade_plot(prof_arr, time, line_dict, y_shift=0.5):\n",
    "    \"\"\" prof_arr, [prof_num, prof_val] - 2d numpy array with dF/F profiles\n",
    "\n",
    "    \"\"\"\n",
    "    prof_df = prof_arr[:,1,:]\n",
    "    time_bar = int(200 * (max(time)/len(time)) )\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    shift = 0\n",
    "    for i in range(prof_df.shape[0]):\n",
    "        plt.plot(time, prof_df[i]+shift, alpha=.5, label=f'ROI {i+1}')\n",
    "        shift += y_shift\n",
    "\n",
    "    for line_name in line_dict:\n",
    "        line_lim = line_dict[line_name]\n",
    "        plt.plot(line_lim, [-0.4] * len(line_lim), label=line_name, linewidth=4)\n",
    "\n",
    "    plt.vlines(x=[-20], ymin=[-0.2], ymax=[0.8], linewidth=3, color='k')\n",
    "    plt.text(x=-30, y=-0.2, s=\"100% ΔF/F\", size=15, rotation=90.)\n",
    "\n",
    "    plt.hlines(y=[-0.75], xmin=[-10], xmax=[time_bar-10], linewidth=3, color='k')\n",
    "    plt.text(x=30, y=-1.15, s=\"200 s\", size=15)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b108137",
   "metadata": {},
   "source": [
    "#### Data uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933a8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_name = 'A0005'\n",
    "samp_path = os.path.join(''.join(sys.path[0].split('neuro')), 'data_neuro', samp_name)\n",
    "\n",
    "# sample YAML metadata file uploading\n",
    "with open(f'{samp_path}/{samp_name}_stim.yaml') as f:\n",
    "    samp_meta = yaml.safe_load(f)\n",
    "\n",
    "# sample data frame uploading\n",
    "pre_df = pd.read_csv(f'{samp_path}/{samp_name}_pre_comp_df.csv')\n",
    "pre_arr, _ = norm_prof_to_arr(pre_df)\n",
    "print(pre_arr.shape)\n",
    "print(pre_df.head())\n",
    "ctrl_plot(pre_df)\n",
    "\n",
    "# post_path = f'{samp_path}/{samp_name}_post_comp.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "963f0c3f",
   "metadata": {},
   "source": [
    "#### Sample global parameters, detection & measurment options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time parameters (from meta file)\n",
    "total_reg_t = samp_meta['Reg_time']\n",
    "frame_time = total_reg_t / max(pre_df['index'])\n",
    "time_line = np.linspace(0, total_reg_t, num=max(pre_df['index'])+1)\n",
    "\n",
    "# treatment parameters (from meta file)\n",
    "treatment_dict = {}\n",
    "for samp in samp_meta['Events']:\n",
    "    treat_name = f\"{samp['Type']}_{samp['Freq']}\"\n",
    "    start_time = samp['Time']/1000\n",
    "    end_time = start_time + samp['Duration']\n",
    "    treatment_dict.update({treat_name:[start_time, end_time]})\n",
    "application_lines_dict = {t:np.linspace(treatment_dict[t][0], treatment_dict[t][1]) for t in treatment_dict}\n",
    "\n",
    "# detection options\n",
    "min_distance_sec = 3\n",
    "min_distance_frames = int(min_distance_sec / frame_time)\n",
    "if min_distance_frames < 1:\n",
    "    min_distance_frames = 1\n",
    "\n",
    "width_sec = [1, 200]  # [min, max]\n",
    "width_frames = np.asanyarray([width_sec[0]/frame_time, width_sec[1]/frame_time], dtype=int)\n",
    "\n",
    "plat_sec = [1, 500]  # [min, max]\n",
    "plat_frames = np.asarray([plat_sec[0]/frame_time, plat_sec[1]/frame_time], dtype=int)\n",
    "plat_frames[plat_frames == 0] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e54a403f",
   "metadata": {},
   "source": [
    "#### Ctrl. ΔF/F profiles plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_plot(pre_arr, time_line, application_lines_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a02a26e",
   "metadata": {},
   "source": [
    "## Peaks detection with custom detector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25ebf737",
   "metadata": {},
   "source": [
    "#### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74015b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel section\n",
    "x = np.linspace(0, 10, 100)\n",
    "\n",
    "# exp kernel\n",
    "y = 1 / np.exp(np.linspace(0, 9, 90))\n",
    "y = np.concatenate([np.zeros(10), y])\n",
    "\n",
    "tau = 0.75\n",
    "y_1 = 1 / np.exp(np.linspace(0, 9, 90) * tau)\n",
    "y_1 = np.concatenate([np.zeros(10), y_1])\n",
    "\n",
    "# square kernel\n",
    "y_2 = np.zeros(100)\n",
    "y_2[10:60] = 1\n",
    "\n",
    "\n",
    "# conv section\n",
    "one_comp_profile = pre_arr[2]  # <=================================\n",
    "prof_raw = one_comp_profile[0]\n",
    "\n",
    "\n",
    "print(one_comp_profile.shape)\n",
    "\n",
    "\n",
    "conv_raw = ndimage.convolve1d(prof_raw, y_1)\n",
    "conv_1 = ndimage.convolve1d(prof_raw, y_1)\n",
    "\n",
    "# scaling 0-1\n",
    "prof = preprocessing.minmax_scale(prof_raw)\n",
    "conv = preprocessing.minmax_scale(conv_raw)\n",
    "\n",
    "# convolved vector shift compensation\n",
    "prof_max = np.argmax(prof[:int(prof.shape[0]/4)])\n",
    "conv_max = np.argmax(conv[:int(conv.shape[0]/4)])\n",
    "# conv_shift = prof_max - conv_max\n",
    "# print(conv_shift)\n",
    "conv = np.concatenate([np.zeros(90), conv[:-90]])\n",
    "\n",
    "# debug plot\n",
    "debug_plot = True\n",
    "if debug_plot:\n",
    "    # kernel plot\n",
    "    plt.plot(x, y, color='b', alpha=.5, label='tau=1')\n",
    "    plt.plot(x, y_1, color='r', alpha=.5, label=f'tau={tau}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # conv plot\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(prof, color='b', alpha=.25, label='raw')\n",
    "    plt.plot(conv, color='r', alpha=.75, label='tau=1')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# # detection\n",
    "# peaks, properties = signal.find_peaks(conv,\n",
    "#                                       height=None,\n",
    "#                                       threshold=None,\n",
    "#                                       distance=min_distance_frames,\n",
    "#                                       wlen=100,\n",
    "#                                       prominence=0.04,\n",
    "#                                       rel_height=0.5,\n",
    "#                                       width=width_frames)\n",
    "\n",
    "# peaks plot without plat det\n",
    "prof_peaks_plot(input_profile=conv, peaks_i=peaks, peaks_prop=properties, x_time=time_line, add_profile=prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa49dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plateau peaks merging\n",
    "plat_th = 0.5\n",
    "bases_arr = np.asarray([conv[properties['left_bases']],conv[properties['right_bases']]]).T\n",
    "\n",
    "if np.any(bases_arr < plat_th):\n",
    "    bases_i_arr = np.asarray([properties['left_bases'],properties['right_bases']]).T\n",
    "    bad_bases = np.asarray([np.zeros(2) if all(i<plat_th) else i for i in bases_arr])  # np.asarray([i for i in bases_arr if any(i>=0.5)])\n",
    "\n",
    "    # plat extraction\n",
    "    plat_i = np.asarray([True if any(b>=plat_th) else False for b in bases_arr])\n",
    "    plat_mask,_ = ndimage.label(plat_i)\n",
    "    plat_ends = [[bases_arr[plat_mask == p_n][0][0], bases_arr[plat_mask == p_n][-1][1]] for p_n in np.trim_zeros(np.unique(plat_mask))]  # plat ends extraction\n",
    "    plat_ends_i = [[bases_i_arr[plat_mask == p_n][0][0], bases_i_arr[plat_mask == p_n][-1][1]] for p_n in np.trim_zeros(np.unique(plat_mask))]  # plat ends index extraction\n",
    "\n",
    "    # print(plat_ends_i)\n",
    "    # print(plat_ends)\n",
    "\n",
    "    # plat features calc\n",
    "    prom_win_scale = 2\n",
    "    plat_prom_win = max([b[1]-b[0] for b in plat_ends_i]) * prom_win_scale\n",
    "    plat_peaks = [np.argmax(conv[b[0]:b[1]])+b[0] for b in plat_ends_i]\n",
    "\n",
    "    print(plat_ends_i)\n",
    "    print(plat_prom_win)\n",
    "\n",
    "\n",
    "    plat_prom = signal.peak_prominences(conv, plat_peaks, wlen=plat_prom_win)\n",
    "    plat_w = signal.peak_widths(conv, plat_peaks, rel_height=0.5, prominence_data=plat_prom)\n",
    "\n",
    "    print(plat_prom)\n",
    "\n",
    "    # prop update\n",
    "    # full_peaks = np.delete(peaks, plat_i)\n",
    "    # full_peaks = np.concatenate((full_peaks, plat_peaks))\n",
    "    peaks = np.concatenate((np.delete(peaks, plat_i), plat_peaks))\n",
    "\n",
    "    properties.update({'prominences':np.concatenate((np.delete(properties['prominences'], plat_i), plat_prom[0]))})\n",
    "    properties.update({'left_bases':np.concatenate((np.delete(properties['left_bases'], plat_i), plat_prom[1]))})\n",
    "    properties.update({'right_bases':np.concatenate((np.delete(properties['right_bases'], plat_i), plat_prom[2]))})\n",
    "    properties.update({'widths':np.concatenate((np.delete(properties['widths'], plat_i), plat_w[0]))})\n",
    "    properties.update({'width_heights':np.concatenate((np.delete(properties['width_heights'], plat_i), plat_w[1]))})\n",
    "    properties.update({'left_ips':np.concatenate((np.delete(properties['left_ips'], plat_i), plat_w[2]))})\n",
    "    properties.update({'right_ips':np.concatenate((np.delete(properties['right_ips'], plat_i), plat_w[3]))})\n",
    "\n",
    "# peaks plot with plat det\n",
    "prof_peaks_plot(input_profile=conv, peaks_i=peaks, peaks_prop=properties, x_time=time_line, add_profile=prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class p_det():\n",
    "    \"\"\" Detector class\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caiman",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "29b2b70970d54890c2ecd25b23b96024fbcef9a6a76cf3bb8bb780f56e497e25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
